# Intro To Deep Learning - Learning the use of Tensorflow and Keras

## Single Neuron 
- Deep learning is an approach to machine learning characterized by deep stacks of computation. the depth of computation has enabled deep learning models to break the complex and challenging real world data sets. Single Neuron is a linear unit (building block) that can have some input(s) and outup(s). the formulla `y = wx+b` depicts the working of a neuron. while w in the input x is the qty of input and b is bias value and y is the output. In case of mutliple inputs the formulla will look like this: `y = w0x0 + w1x1 + w2x2 + w3x3 + b`. while w0,w1,w2,w3 are different inputs.  Easiest way to create models in Keras is Keras.Sequential. 

## Deep Neural Networks 
- Building up complex network from simpler functional units. Neural Networks are orgnaise their neurons into layers, the liner units having common inputs are collected together into dense layers. There can be multiple layers and each layer is a transformation that takes us closer to the solution. An activation function is some function that is applied to the layer's outputs(its activations). the most common is rectifier function `max(0,X)`. Rectifier function has a graph that has the -ve values rectified to Zero. When we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU. ReLU application formulla `max(0 , w * x + b )`

## Stochastic Gradient Descent
- We compile model by adding an optimizer and loss function. Optimiser is the function that tells how to solve the problem and example function is `Adam`. Loss function as we know helps in validation and gives us an idea about the accuracy and an exmample is `mae`. There are multiple Optimizer functions and all of them virutally belong to Stochastic Gradient Descent. They peform their job in 3 steps `1 - Sample some traning data and run it though the network to make predictions` `2 - Measure the loss between prediction and true value` `3 - Finally adjust the weights in a direction to make the losses smaller`, then just repeat 1-2 over and over again untill the loss is smaller or won't decrease further.